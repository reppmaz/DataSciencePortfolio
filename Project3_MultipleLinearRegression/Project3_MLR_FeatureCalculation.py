#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue May 23 12:52:46 2023

@author: reppmazc

This script processes functional connectivity data derived from a multi-site
brain imaging study, including two resting state scans (rest1 = pre stress,
rest2 = post stress) which are flanking a stress task (ScanSTRESS).

Input: .tsv files as generated by e.g. halfpipe, containing pearson correlation
vlaues between all brain regions (defined by a brain atals - brainnetome atlas in this case)

The script imolements the following steps:
- defines a function for a Fisher's Z-transformation to each given correlation
  coefficient, enhancing the stability of variance for correlation-based data.
- iterates through files in the specified directory to extract participant IDs and task sequences from filenames
- reads correlation matrices stored in TSV files (containing Pearson correlation
  coefficients between different brain regions) or each participant and task
- extracts specific submatrices corresponding to three large-scale brain networks:
  Salience Network (SN), Default Mode Network (DMN), Central Executive Network (CEN)
  as well as Cross-network interactions such as SN x DMN and SN x CEN.
- Fisher's Z-transformation is applied to these matrices, and the mean connectivity
 for each network and task is calculated and stored.
- data is reshaped into a wide format where each row represents a participant, and columns represent average connectivity measures for each network across tasks.
- column names are standardized to more descriptive phase names (pre, stress, post).
- calculates reactivity and recovery features by comparing connectivity values between different phases (e.g., stress vs. pre-stress).
- saves features as csv

"""
#--------
# imports
#--------
import glob
import pandas as pd
import os
import numpy as np
from sklearn.impute import IterativeImputer
from scipy.spatial import distance
import matplotlib.pyplot as plt

## defining lists
tasks = ["rest1", "stress", "rest2"]

#------------------------
# Fisher z transformation
#------------------------
# function for Fisher z transformation
def fisher(x):
    z = 0.5 * np.log((1+x)/(1-x))
    return z

#-----------------------------------------------------------------
# CALCULATING INDIVIDUAL AVERAGE CONNECTIVITY PER TASK AND NETWORK
#-----------------------------------------------------------------
mean_conn = pd.DataFrame() ## result
IDs = [] ## participant ID
seq = [] ## rest1, stress task, rest2
mean_conn_all = [] ##
mean_conn_SN = []
mean_conn_DMN = []
mean_conn_CEN = []
mean_conn_SNDMN = []
mean_conn_SNCEN = []
mean_conn_CENDMN = []

directory = "/path_to_data/"

#-------------------
# getting basic info
#-------------------
## extracting ID and sequence from filenames
for filename in os.listdir(directory):
    if filename.endswith("_correlation_matrix.tsv"):
        subject = os.path.normpath(filename[4:9]) #grabbing participant ID from filename
        sequence = os.path.normpath(filename[15:20])#grabbing sequence name
        seq.append(sequence)
        IDs.append(subject)
    else:
        continue

# Print information about unique IDs
unique_IDs = set(IDs)
print(f"Total number of IDs: {len(IDs)}")
print(f"Number of unique IDs: {len(unique_IDs)}")

mean_conn["ID"] = IDs
mean_conn["task"] = seq
mean_conn["site"] = mean_conn["ID"].str[1].map({"1":"Site1", "2":"Site2", "3":"Site3", "4":"Site4", "5":"Site5"})
## add missing s to stress
mean_conn["task"] = mean_conn["task"].replace("stres", "stress")

# Filtering for unique IDs with a stress task
stress_IDs = mean_conn[mean_conn['task'] == 'stress']['ID'].unique()

# Printing information
print(f"Total number of IDs: {len(IDs)}")
print(f"Number of unique IDs: {len(set(IDs))}")
print(f"Number of unique IDs with a stress task: {len(stress_IDs)}")
print("Unique IDs with a stress task:", stress_IDs)

#---------------------------------------------------------------------------
# calculating average correlation matrices per participant, network and task
#---------------------------------------------------------------------------
## iterating through files:
for file in glob.glob('/path_to_data/sub-*_task-*_correlation_matrix.tsv'):
    filename = os.fsdecode(file)
    
# the correlation matrixes contain the pearson correlations between all brain regions
# each network consists of different regions, therefore columns and rows(i.e.,
# brain regions have to be selected for each network separately

## loading connectivity matrix (person r):
    matrix = pd.read_csv(filename, sep='\t', dtype = float, header = None)

# salience network (SN)
    matrix_SN = matrix.iloc[[91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,
                            293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,400,401],
                           [91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,293,294,295,296,297,298,299,
                            300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,400,401]]
    #matrix_SN =  matrix_SN.applymap(fisher)
    mcn_SN = matrix_SN.mean().mean()
    mean_conn_SN.append(mcn_SN)

# default mode network (DMN)
    matrix_DMN = matrix.iloc[[148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,
                            171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,
                            193,194,195,196,197,198,199,
                            361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,
                            384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,402,403],
                           [148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,
                            171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,
                            193,194,195,196,197,198,199,
                            361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,
                            384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,402,403]]
    matrix_DMN = matrix_DMN.replace(1,0)
    matrix_DMN =  matrix_DMN.applymap(fisher)
    matrix_DMN = matrix_DMN.replace(0,1)
    mcn_DMN = matrix_DMN.mean().mean()
    mean_conn_DMN.append(mcn_DMN)
    
# central executive network (CEN)
    matrix_CEN = matrix.iloc[[126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,
                          331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,
                          354,355,356,357,358,359,360],
                          [126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,
                          331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,
                          354,355,356,357,358,359,360]]
    matrix_CEN = matrix_CEN.replace(1,0)
    matrix_CEN =  matrix_CEN.applymap(fisher)
    matrix_CEN = matrix_CEN.replace(0,1)
    mcn_CEN = matrix_CEN.mean().mean()
    mean_conn_CEN.append(mcn_CEN)

# Salience network (SN) x central executive network (CEN)
    matrix_SNCEN = matrix.iloc[[91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,
                            293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,400,401],
                            [126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,
                            331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,
                            354,355,356,357,358,359,360]]
    matrix_SNCEN =  matrix_SNCEN.applymap(fisher)
    mcn_SNCEN = matrix_SNCEN.mean().mean()
    mean_conn_SNCEN.append(mcn_SNCEN)

# Salience network (SN) x default mode network (DMN)
    matrix_SNDMN = matrix.iloc[[91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,
                          293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,400,401],
                         [148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,
                          171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,
                          193,194,195,196,197,198,199,
                          361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,
                          384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,402,403]]
    matrix_SNDMN =  matrix_SNDMN.applymap(fisher)
    mcn_SNDMN = matrix_SNDMN.mean().mean()
    mean_conn_SNDMN.append(mcn_SNDMN)

# Central executive network (CEN) x default mode network (DMN)
    matrix_CENDMN = matrix.iloc[[126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,
                          331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,
                          354,355,356,357,358,359,360],
                         [148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,
                          171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,
                          193,194,195,196,197,198,199,
                          361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,
                          384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,402,403]]
    matrix_CENDMN =  matrix_CENDMN.applymap(fisher)
    mcn_CENDMN = matrix_CENDMN.mean().mean()
    mean_conn_CENDMN.append(mcn_CENDMN)

# Assuming mean_conn_SN, mean_conn_DMN, etc., are lists of data
mean_conn["mean_conn_SN"] = mean_conn_SN
mean_conn["mean_conn_DMN"] = mean_conn_DMN
mean_conn["mean_conn_CEN"] = mean_conn_CEN
mean_conn["mean_conn_SNCEN"] = mean_conn_SNCEN
mean_conn["mean_conn_SNDMN"] = mean_conn_SNDMN
mean_conn["mean_conn_CENDMN"] = mean_conn_CENDMN

# Reshape the dataframe
mean_conn_wide = mean_conn.pivot(index='ID', columns='task', values=[col for col in mean_conn.columns if col.startswith("mean_conn_")])

# Adjust the column names
mean_conn_wide.columns = ['_'.join(col).strip() for col in mean_conn_wide.columns.values]

# Reset the index
mean_conn_wide.reset_index(inplace=True)

# Save the new wide dataframe
mean_conn_wide.to_csv("path_to_save/AvgCorr_wide.csv", index=False)

#----------------------------------
# CALCULATING CONNECTIVITY FEATURES
#----------------------------------
FC = mean_conn_wide.copy()

# Function to rename columns
def rename_columns(col_name):
    # Mapping for the phases
    phase_mapping = {
        'rest1': 'pre',
        'rest2': 'post',
        'stress': 'stress'
    }
    
    # Split the original column name
    parts = col_name.split('_')
    
    # Check if the column name matches the pattern "mean_conn_..."
    if len(parts) >= 3 and parts[0] == "mean" and parts[1] == "conn":
        # Assemble the new column name
        network = parts[2]
        phase = phase_mapping[parts[3]]
        return f"{network}_{phase}"
    
    # Return the original name if no change is needed
    return col_name

# Rename columns
new_columns = {col: rename_columns(col) for col in FC.columns}
FC.rename(columns=new_columns, inplace=True)

# Identify unique networks
networks = set(col.split('_')[0] for col in FC.columns if any(phase in col for phase in ['pre', 'stress', 'post']))

# Calculate [network]_post - [network]_pre for each network
for network in networks:
    FC[f'{network}_reactivity'] = FC[f'{network}_stress'] - FC[f'{network}_pre']
    FC[f'{network}_recovery'] = FC[f'{network}_stress'] - FC[f'{network}_post']

FC.to_csv("path_to_save/Features.csv")